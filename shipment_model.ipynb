{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCwFPIMLmHJ0X5QtmyHG8a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikf378/About/blob/main/shipment_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mMnqAtmbxy0",
        "outputId": "0d4301b5-276b-40ce-eabe-c926868c432a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define data paths for real data\n",
        "data_paths = {\n",
        "    \"stationary\": {\"path\": r\"/content/drive/MyDrive/smart_shipment/0 stationary\", \"target\": 0},\n",
        "    \"moving\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/1 moving\", \"target\": 1},\n",
        "    \"picked\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/2 picked\", \"target\": 2},\n",
        "    \"wrong\":      {\"path\": r\"/content/drive/MyDrive/smart_shipment/3 wrong\", \"target\": 3},\n",
        "    \"thrown\":     {\"path\": r\"/content/drive/MyDrive/smart_shipment/4 thrown\", \"target\": 4}\n",
        "}\n",
        "\n",
        "# Parameters for windowing (from synthetic generation approach)\n",
        "window_size = 50\n",
        "sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']  # Correct sensor columns\n",
        "\n",
        "# Collect features and labels from real data with window-based feature extraction\n",
        "data = []\n",
        "labels = []\n",
        "all_data_dfs = []  # For optional combined CSV saving\n",
        "\n",
        "for label_name, info in data_paths.items():\n",
        "    files = glob.glob(os.path.join(info[\"path\"], \"*.csv\"))\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        # Add target column for optional saving\n",
        "        df[\"target\"] = info[\"target\"]\n",
        "        all_data_dfs.append(df)\n",
        "\n",
        "        # Select sensor data\n",
        "        if not all(col in df.columns for col in sensor_cols):\n",
        "            print(f\"Skipping {file}: Missing sensor columns.\")\n",
        "            continue\n",
        "        data_df = df[sensor_cols]\n",
        "\n",
        "        # Convert to numeric and drop NaNs\n",
        "        data_df = data_df.apply(pd.to_numeric, errors='coerce')\n",
        "        data_df = data_df.dropna()\n",
        "\n",
        "        # Extract windows\n",
        "        num_windows = len(data_df) // window_size\n",
        "        for i in range(num_windows):\n",
        "            window = data_df.iloc[i * window_size : (i + 1) * window_size]\n",
        "            acc = window[['acc_x', 'acc_y', 'acc_z']].values\n",
        "            gyro = window[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
        "            features = np.concatenate([\n",
        "                acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "            ])\n",
        "            data.append(features)\n",
        "            labels.append(info[\"target\"])\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Optional: Save combined raw dataset (before windowing)\n",
        "combined_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)\n",
        "print(f\"Combined dataset saved. Shape: {combined_df.shape}\")\n",
        "\n",
        "# If no data was collected (e.g., insufficient windows), fallback to synthetic\n",
        "if len(X) == 0:\n",
        "    print(\"No real data windows found. Generating synthetic data.\")\n",
        "    def generate_synthetic_data(num_samples_per_class=500, num_classes=5, window_size=50):\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for class_id in range(num_classes):\n",
        "            for _ in range(num_samples_per_class // window_size):\n",
        "                if class_id == 0:  # stationary: small noise around gravity\n",
        "                    acc = np.random.normal([0, 0, 1], 0.01, size=(window_size, 3))\n",
        "                    gyro = np.random.normal(0, 0.1, size=(window_size, 3))\n",
        "                elif class_id == 1:  # moving: sinusoidal motion\n",
        "                    t = np.linspace(0, 2 * np.pi, window_size)\n",
        "                    acc = np.column_stack([np.sin(t)*0.5, np.cos(t)*0.3, np.ones(window_size) + np.sin(t)*0.1])\n",
        "                    gyro = np.column_stack([np.cos(t)*5, np.sin(t)*3, np.random.normal(0, 1, window_size)])\n",
        "                elif class_id == 2:  # picked: sudden upward acceleration\n",
        "                    acc = np.column_stack([np.random.normal(0, 0.05, window_size),\n",
        "                                           np.random.normal(0, 0.05, window_size),\n",
        "                                           np.linspace(1, 1.5, window_size) + np.random.normal(0, 0.1, window_size)])\n",
        "                    gyro = np.random.normal(0, 2, size=(window_size, 3))\n",
        "                elif class_id == 3:  # wrong position: tilted\n",
        "                    acc = np.random.normal([0.5, 0.5, 0.7], 0.05, size=(window_size, 3))\n",
        "                    gyro = np.random.normal([10, 10, 0], 1, size=(window_size, 3))\n",
        "                elif class_id == 4:  # thrown: high acceleration and rotation\n",
        "                    acc = np.random.normal(0, 1, size=(window_size, 3)) + np.linspace(0, 2, window_size)[:, None]\n",
        "                    gyro = np.random.normal(0, 20, size=(window_size, 3))\n",
        "\n",
        "                # Flatten the window into features\n",
        "                features = np.concatenate([\n",
        "                    acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "                ])\n",
        "                data.append(features)\n",
        "                labels.append(class_id)\n",
        "\n",
        "        return np.array(data), np.array(labels)\n",
        "\n",
        "    X, y = generate_synthetic_data()\n",
        "\n",
        "# Preprocess: Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset\n",
        "class SensorDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = SensorDataset(X_train, y_train)\n",
        "test_dataset = SensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Simple MLP Model\n",
        "class ShipmentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShipmentClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X.shape[1]  # 12 features (mean/std for 6 sensors)\n",
        "num_classes = 5\n",
        "model = ShipmentClassifier(input_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'shipment_model.pkl')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.numpy())\n",
        "        y_true.extend(labels.numpy())\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "new_df = pd.read_csv(\"/content/drive/MyDrive/test_1/THROWN.csv\")\n",
        "\n",
        "# Preprocess the same way as training data\n",
        "# Drop target column if it exists\n",
        "if 'target' in new_df.columns:\n",
        "    new_df = new_df.drop(columns=['target'], errors='ignore')\n",
        "\n",
        "# Select only sensor columns and convert to numeric, dropping NaNs\n",
        "new_df = new_df[sensor_cols].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "\n",
        "# Apply window-based feature extraction to new data\n",
        "new_data = []\n",
        "num_windows = len(new_df) // window_size\n",
        "\n",
        "for i in range(num_windows):\n",
        "    window = new_df.iloc[i * window_size : (i + 1) * window_size]\n",
        "    acc = window[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    gyro = window[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
        "    features = np.concatenate([\n",
        "        acc.mean(axis=0), acc.std(axis=0), gyro.mean(axis=0), gyro.std(axis=0)\n",
        "    ])\n",
        "    new_data.append(features)\n",
        "\n",
        "# Convert to array\n",
        "new_data_array = np.array(new_data)\n",
        "\n",
        "\n",
        "# Scale the new data using the same scaler fitted on the training data\n",
        "# This assumes the scaler object is still available from the previous cell\n",
        "if 'scaler' in globals():\n",
        "    new_data_scaled = scaler.transform(new_data_array)\n",
        "    new_data_scaled_tensor = torch.tensor(new_data_scaled, dtype=torch.float32)\n",
        "else:\n",
        "    print(\"Scaler not found. Cannot scale new data. Using unscaled data.\")\n",
        "    new_data_scaled_tensor = torch.tensor(new_data_array, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(new_data_scaled_tensor)\n",
        "    _, predicted_classes = torch.max(outputs, 1)\n",
        "\n",
        "# The predicted_classes tensor contains the predicted class index for each window of the new data\n",
        "print(\"Predicted classes:\", predicted_classes.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuZhlmDVb2JQ",
        "outputId": "59a72026-aec8-42e0-e522-8a396fa983ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved. Shape: (16033, 7)\n",
            "Epoch [10/50], Loss: 0.5948\n",
            "Epoch [20/50], Loss: 0.4619\n",
            "Epoch [30/50], Loss: 0.0355\n",
            "Epoch [40/50], Loss: 0.0611\n",
            "Epoch [50/50], Loss: 0.0255\n",
            "Accuracy: 98.31%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        11\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      0.92      0.96        13\n",
            "           3       1.00      1.00      1.00         9\n",
            "           4       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           0.98        59\n",
            "   macro avg       0.98      0.98      0.98        59\n",
            "weighted avg       0.98      0.98      0.98        59\n",
            "\n",
            "Predicted classes: [4 4 4 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this import at the top of your training script\n",
        "import joblib\n",
        "\n",
        "# In your training code, after this line:\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add this line to save the scaler:\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"Scaler saved as scaler.pkl\")"
      ],
      "metadata": {
        "id": "0-bmaT6_V82r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf79e81-c940-4524-f7c0-ee8b8f1d764f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler saved as scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after loading the model\n",
        "test_features = np.random.randn(1, 12)  # Replace with actual features from your training data\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "test_tensor = torch.tensor(test_features_scaled, dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "print(f\"Test prediction: {predicted.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGwBh0hgYdDm",
        "outputId": "3d53d977-e750-4fd0-9b2e-2bd07fd21e7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test prediction: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4TqmCr8gNN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}